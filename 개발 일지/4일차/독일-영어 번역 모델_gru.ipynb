{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iTiPtDSkQLy4",
    "outputId": "9096aad8-a69d-4eb0-f2a1-436707de9474"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple/\n",
      "Requirement already satisfied: torchtext==0.4 in /home/ubuntu/anaconda3/lib/python3.8/site-packages (0.4.0)\n",
      "Requirement already satisfied: torch in /home/ubuntu/anaconda3/lib/python3.8/site-packages (from torchtext==0.4) (1.10.0)\n",
      "Requirement already satisfied: requests in /home/ubuntu/anaconda3/lib/python3.8/site-packages (from torchtext==0.4) (2.26.0)\n",
      "Requirement already satisfied: numpy in /home/ubuntu/anaconda3/lib/python3.8/site-packages (from torchtext==0.4) (1.19.5)\n",
      "Requirement already satisfied: tqdm in /home/ubuntu/anaconda3/lib/python3.8/site-packages (from torchtext==0.4) (4.62.3)\n",
      "Requirement already satisfied: six in /home/ubuntu/anaconda3/lib/python3.8/site-packages (from torchtext==0.4) (1.16.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/anaconda3/lib/python3.8/site-packages (from requests->torchtext==0.4) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ubuntu/anaconda3/lib/python3.8/site-packages (from requests->torchtext==0.4) (2.0.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/anaconda3/lib/python3.8/site-packages (from requests->torchtext==0.4) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ubuntu/anaconda3/lib/python3.8/site-packages (from requests->torchtext==0.4) (1.26.7)\n",
      "Requirement already satisfied: typing-extensions in /home/ubuntu/anaconda3/lib/python3.8/site-packages (from torch->torchtext==0.4) (3.10.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchtext==0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "koPteALdQQAf"
   },
   "outputs": [],
   "source": [
    "# 라이브러리 로딩\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchtext.datasets import TranslationDataset, Multi30k\n",
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FwNWzyHVQRAA"
   },
   "outputs": [],
   "source": [
    "# 필요 tokenizer 설치\n",
    "!python -m spacy download en\n",
    "!python -m spacy download de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "8JPnZ3AoQSJm"
   },
   "outputs": [],
   "source": [
    "# 각 언어에 맞는 tokenizer 불러오기\n",
    "\n",
    "spacy_de = spacy.load('de')\n",
    "spacy_en = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ZcBpeyzQQTY2"
   },
   "outputs": [],
   "source": [
    "def tokenize_de(text):\n",
    "    \"\"\"\n",
    "    독일어 tokenize해서 단어들을 리스트로 만든 후 reverse \n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)][::-1]\n",
    "    \n",
    "def tokenize_en(text):\n",
    "    \"\"\" \n",
    "    영어 tokenize해서 단어들을 리스트로 만들기\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "a8bYxcjKQU42"
   },
   "outputs": [],
   "source": [
    "# Field 선언\n",
    "\n",
    "#input\n",
    "SRC  = Field(tokenize = tokenize_de, init_token= '<sos>', eos_token = '<eos>', lower =True)\n",
    "\n",
    "#output\n",
    "TRG  = Field(tokenize = tokenize_en, init_token= '<sos>', eos_token = '<eos>', lower =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1rth8xJ9QWg7",
    "outputId": "1e3521fe-21d1-40b5-fb8a-06c3a3d91b89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading training.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training.tar.gz: 100%|██████████| 1.21M/1.21M [00:03<00:00, 318kB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading validation.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "validation.tar.gz: 100%|██████████| 46.3k/46.3k [00:00<00:00, 56.1kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading mmt_task1_test2016.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mmt_task1_test2016.tar.gz: 100%|██████████| 66.2k/66.2k [00:00<00:00, 77.7kB/s]\n"
     ]
    }
   ],
   "source": [
    "# torchtext의 Multi30k 데이터 불러오기\n",
    "# exts : 어떤 언어 사용할지 명시 (input 언어를 먼저 씀)\n",
    "# filed = (입력, 출력)\n",
    "\n",
    "train_data, valid_data, test_data = Multi30k.splits(exts=('.de', '.en'), fields=(SRC,TRG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pownGnyoQYB1",
    "outputId": "dd69caa8-48e5-45a3-a158-998ed8296453"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'src': ['.', 'büsche', 'vieler', 'nähe', 'der', 'in', 'freien', 'im', 'sind', 'männer', 'weiße', 'junge', 'zwei'], 'trg': ['two', 'young', ',', 'white', 'males', 'are', 'outside', 'near', 'many', 'bushes', '.']}\n"
     ]
    }
   ],
   "source": [
    "# 데이터 확인\n",
    "# 독일 단어는 역순임.\n",
    "print(vars(train_data.examples[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H5_ttGxSQbyT"
   },
   "source": [
    "## Vocab "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "7thtGPk7QZiY"
   },
   "outputs": [],
   "source": [
    "#최소 2번은 등장하는 단어만 vocab에 포함\n",
    "\n",
    "SRC.build_vocab(train_data, min_freq=2)\n",
    "TRG.build_vocab(train_data, min_freq=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "OydH0DrNQdM5"
   },
   "outputs": [],
   "source": [
    "# BucketIterator 라이브러리를 이용하여 데이터를 iterator로 만듦\n",
    "batch_size = 128\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits((train_data, valid_data, \n",
    "                                                                       test_data), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WtmNvTnSQfQL"
   },
   "source": [
    "## 모델링\n",
    "\n",
    "### encoder\n",
    "- 2 layer RNN\n",
    "- back-bone으로 GRU 사용\n",
    "- Layer 1: 독일어 토큰의 임베딩을 입력으로 받고 은닉상태 출력\n",
    "- Layer 2 : Layer1의 은닉상태를 입력으로 받고 새로운 은닉상태 출력\n",
    "- 각 layer마다 초기 은닉상태 h_0 필요 (0으로 초기화 ?)\n",
    "- 각 layer마다 context vector 'z'를 출력\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "V3BkNuOoQdaB"
   },
   "outputs": [],
   "source": [
    "# encoder \n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    seq2seq의 encoder\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout = 0.2):\n",
    "        \"\"\"\n",
    "        \n",
    "        파라미터 \n",
    "        ---\n",
    "        input_dim : int\n",
    "            input 데이터의 차원(= vocab size)\n",
    "        emb_dim : int\n",
    "            embedding layer의 차원\n",
    "        hid_dim : int\n",
    "            은닉 상태의 차원\n",
    "        n_layers : int\n",
    "            RNN 안의 레이어 개수 (여기선 2개)\n",
    "        dropout : float\n",
    "            사용할 드롭아웃의 비율 (오버피팅 방지하는 정규화 방법)\n",
    "        \n",
    "        return\n",
    "        ---\n",
    "        hidden : [[n layers * n directions, batch size, hid dim]]\n",
    "            encoder의 hidden state. decoder의 입력으로 사용됨\n",
    "        \"\"\"\n",
    "       \n",
    "        super().__init__()\n",
    "\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.rnn = nn.GRU(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        \n",
    "        #src = [src len, batch_size)]\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        #embeded = [src len, batch size, emb dim]\n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "        # outputs = [src len, batch size, hid dim * n directions]\n",
    "        # hidden = [n layers * n directions, batch size, hid dim]\n",
    "\n",
    "        \n",
    "        return hidden\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TQYsDOIsQoJv"
   },
   "source": [
    "### decoder\n",
    "\n",
    "- Layer 1 : 직전 time-stamp로 부터 은닉 상태(s)와 cell state를 받고, 이들과 embedded token인 y_t를 입력으로 받아 새로운 은닉상태와 cell state를 만들어냄\n",
    "- Layer 2 : Layer 2의 은닉 상태(s)와 Layer 2에서 직전 time-stamp의 은닉 상태(s)와 cell state를 입력으로 받아 새로운 은닉 상태와 cell state를 만들어냄\n",
    "- Decoder Layer1의 `첫 은닉상태(s)와 cell state` = `context vector (z)` = `Encoder Layer 1의 마지막 은닉상태(h)와 cell state`\n",
    "- Decoder RNN/LSTM의 맨 위 Layer의 은닉 상태를 Linear Layer인 f에 넘겨서 다음 토큰이 무엇일지 예측함\n",
    "- 여기서는 GRU를 사용했기 때문에 cell state는 없음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "AyEm_lVFQkM7"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module) : \n",
    "    \"\"\"\n",
    "    seq2seq의 Decoder\n",
    "    \"\"\"\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        \"\"\"\n",
    "\n",
    "        파라미터\n",
    "        ---\n",
    "        output_dim : int\n",
    "            출력 될 데이터의 차원, 타겟 데이터의 임베딩 차원        \n",
    "        emb_dim  : int\n",
    "            embedding layer의 차원\n",
    "        hid_dim : int\n",
    "            은닉 상태의 차원\n",
    "        n_layers : int\n",
    "            RNN 안의 레이어 개수 (여기선 2개)\n",
    "        dropout : float\n",
    "            사용할 드롭아웃의 비율 (오버피팅 방지하는 정규화 방법)\n",
    "\n",
    "        returns\n",
    "        ---\n",
    "        prediction : torch.tensor\n",
    "            현재 sequence에서 생성된 출력 벡터(단어)\n",
    "        hidden : [n layers, batch size, hid dim]\n",
    "            decoder의 hidden state. 다음 decoder로 전달됨.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.output_dim = output_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.emb_dim = emb_dim\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.rnn = nn.GRU(emb_dim, hid_dim, n_layers, dropout=dropout)\n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        # input = [batch size]\n",
    "        # hidden = [n layers * n directions, batch size, hid dim]\n",
    "        # Decoder에서 항상 n directions = 1\n",
    "        # 따라서 hidden = [n layers, batch size, hid dim]\n",
    "        # context = [n layers, batch size, hid dim]\n",
    "        \n",
    "        # input = [1, batch size]\n",
    "        input = input.unsqueeze(0)\n",
    "        \n",
    "        # embedded = [1, batch size, emb dim]\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        \n",
    "        output, hidden = self.rnn(embedded, hidden)\n",
    "        \n",
    "        # output = [seq len, batch size, hid dim * n directions]\n",
    "        # hidden = [n layers * n directions, batch size, hid dim]\n",
    " \n",
    "        # Decoder에서 항상 seq len = n directions = 1 \n",
    "        # 한 번에 한 토큰씩만 디코딩하므로 seq len = 1\n",
    "        # 따라서 output = [1, batch size, hid dim]\n",
    "        # hidden = [n layers, batch size, hid dim]\n",
    "        \n",
    "        # prediction = [batch size, output dim]\n",
    "        prediction = self.fc_out(output.squeeze(0))\n",
    "        \n",
    "        return prediction, hidden\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gIR-En-cQrps"
   },
   "source": [
    "### Seq2seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "OJMCFhp6QqE-"
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    \"\"\"\n",
    "    encoder와 decoder를 이용해 seq2seq 모델을 설계하는 class\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "        # Encoder와 Decoder의 hidden dim이 같아야 함 \n",
    "        assert encoder.hid_dim == decoder.hid_dim, \"encoder와 decoder의 hidden dim이 다름.\"\n",
    "        assert encoder.n_layers == decoder.n_layers, \"encoder와 decoder의 n_layers이 다름.\"\n",
    "\n",
    "    def forward(self, src, trg ,teacher_forcing_ratio = 0.5):\n",
    "        \"\"\" \n",
    "        seq2seq 모델을 통해 예측 값 생성\n",
    "\n",
    "        파라미터\n",
    "        ---\n",
    "        src : [src len, batch size]\n",
    "            input 데이터의 임베딩 차원\n",
    "        trg : [trg len, batch size]\n",
    "            target 데이터의 임베딩 차원\n",
    "        teacher_forcing_ration : float\n",
    "            teacher forcing의 비율\n",
    "        \n",
    "        returns\n",
    "        ---\n",
    "        outputs : [trg len, batch size, output dim]\n",
    "            seq2seq를 통해 생성된 단어의 벡터 \n",
    "        \"\"\"\n",
    "\n",
    "        # \n",
    "        # trg = [trg len, batch size]\n",
    "\n",
    "        trg_len = trg.shape[0]\n",
    "        batch_size = trg.shape[1]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        # decoder 결과를 저장할 텐서\n",
    "        outputs = torch.zeros([trg_len,batch_size , trg_vocab_size])\n",
    "\n",
    "        # encoder의 마지막 은닉 상태가 Deocder의 초기 은닉상태로 쓰임\n",
    "        hidden = self.encoder(src)\n",
    "\n",
    "        # decoder에 들어갈 첫 input은 <sos>토큰\n",
    "        input = trg[0,:]\n",
    "\n",
    "        #target length만큼 반복\n",
    "        # range(0,trg_len)이 아니라 range(1,trg_len)인 이유 : 0번째 trg는 항상 <sos>라서 그에 대한 output도 항상 0\n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden = self.decoder(input, hidden)\n",
    "            outputs[t] = output\n",
    "\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "\n",
    "            # 확률 가장 높게 예측한 토큰\n",
    "            top1 = output.argmax(1)\n",
    "\n",
    "            #teacher_force = 1 = true 면 trg[t]를 아니면 top1을 input으로 사용\n",
    "            input = trg[t] if teacher_force else top1\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "Tb_M8vsjQtpx"
   },
   "outputs": [],
   "source": [
    "input_dim = len(SRC.vocab)\n",
    "output_dim = len(TRG.vocab)\n",
    "\n",
    "# Encoder embedding dim \n",
    "enc_emb_dim = 256\n",
    "\n",
    "# Decoder embedding dim \n",
    "dec_emb_dim = 256\n",
    "\n",
    "hid_dim = 512\n",
    "n_layers = 2\n",
    "\n",
    "enc_dropout = 0.5\n",
    "dec_dropout = 0.5\n",
    "\n",
    "enc = Encoder(input_dim, enc_emb_dim, hid_dim, n_layers,enc_dropout)\n",
    "dec = Decoder(output_dim, dec_emb_dim, hid_dim, n_layers,dec_dropout)\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "model = Seq2Seq(enc, dec, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "vbzuZdwfQu6V"
   },
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    \"\"\"\n",
    "    가중치 초기화\n",
    "    \"\"\"\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "model = model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mo9BvRevQx3N"
   },
   "source": [
    "### Opimizer/ Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "f4YeRnWdQwVB"
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# <pad> 토큰의 index를 넘겨 받으면 오차를 계산하지 않고 ignore하기\n",
    "# <pad> = padding\n",
    "trg_pad_idx = TRG.vocab.stoi[TRG.pad_token]\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = trg_pad_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R2yay9d4Q0Dr"
   },
   "source": [
    "## 학습 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "kxo27aItQzrh"
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \"\"\"\n",
    "    모델을 학습하는 코드\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    epoch_loss=0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        src = batch.src.to(device) # [25,128]\n",
    "        trg = batch.trg.to(device) # [29,128]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(src, trg).to(device)\n",
    "        \n",
    "        # trg = [trg len, batch size]\n",
    "        # output = [trg len, batch size, output dim]\n",
    "        output_dim = output.shape[-1]\n",
    "        \n",
    "        # loss 함수는 2d input으로만 계산 가능 \n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg = trg[1:].view(-1)\n",
    "        \n",
    "        # trg = [(trg len-1) * batch size]\n",
    "        # output = [(trg len-1) * batch size, output dim)]\n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        # 기울기 폭발 막기 위해 clip\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss+=loss.item()\n",
    "        \n",
    "    return epoch_loss/len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "F81t03nLQ2iS"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \"\"\"\n",
    "    학습된 모델을 평가하는 코드\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(iterator):\n",
    "            src = batch.src.to(device)\n",
    "            trg = batch.trg.to(device)\n",
    "            \n",
    "            # teacher_forcing_ratio = 0 (아무것도 알려주면 안 됨)\n",
    "            output = model(src, trg, 0).to(device)\n",
    "            \n",
    "            # trg = [trg len, batch size]\n",
    "            # output = [trg len, batch size, output dim]\n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "            \n",
    "            # trg = [(trg len - 1) * batch size]\n",
    "            # output = [(trg len - 1) * batch size, output dim]\n",
    "            \n",
    "            loss = criterion(output, trg)\n",
    "            \n",
    "            epoch_loss+=loss.item()\n",
    "        \n",
    "        return epoch_loss/len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "QvVCBUrWQ4No"
   },
   "outputs": [],
   "source": [
    "# function to count training time\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "pqjIpsoRQ6Iw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 2m 13s\n",
      "\tTrain Loss: 4.914 | Train PPL: 136.126\n",
      "\t Val. Loss: 4.716 |  Val. PPL: 111.745\n",
      "Epoch: 02 | Time: 2m 12s\n",
      "\tTrain Loss: 4.111 | Train PPL:  61.006\n",
      "\t Val. Loss: 4.442 |  Val. PPL:  84.934\n",
      "Epoch: 03 | Time: 2m 10s\n",
      "\tTrain Loss: 3.653 | Train PPL:  38.572\n",
      "\t Val. Loss: 4.008 |  Val. PPL:  55.063\n",
      "Epoch: 04 | Time: 2m 12s\n",
      "\tTrain Loss: 3.338 | Train PPL:  28.161\n",
      "\t Val. Loss: 3.785 |  Val. PPL:  44.046\n",
      "Epoch: 05 | Time: 2m 11s\n",
      "\tTrain Loss: 3.074 | Train PPL:  21.624\n",
      "\t Val. Loss: 3.662 |  Val. PPL:  38.928\n",
      "Epoch: 06 | Time: 2m 11s\n",
      "\tTrain Loss: 2.887 | Train PPL:  17.935\n",
      "\t Val. Loss: 3.572 |  Val. PPL:  35.601\n",
      "Epoch: 07 | Time: 2m 14s\n",
      "\tTrain Loss: 2.721 | Train PPL:  15.189\n",
      "\t Val. Loss: 3.518 |  Val. PPL:  33.713\n",
      "Epoch: 08 | Time: 2m 13s\n",
      "\tTrain Loss: 2.579 | Train PPL:  13.187\n",
      "\t Val. Loss: 3.485 |  Val. PPL:  32.636\n",
      "Epoch: 09 | Time: 2m 13s\n",
      "\tTrain Loss: 2.431 | Train PPL:  11.373\n",
      "\t Val. Loss: 3.535 |  Val. PPL:  34.288\n",
      "Epoch: 10 | Time: 2m 9s\n",
      "\tTrain Loss: 2.350 | Train PPL:  10.482\n",
      "\t Val. Loss: 3.505 |  Val. PPL:  33.294\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JNi0uH1kRNAY"
   },
   "source": [
    "# Reference \n",
    "- https://codlingual.tistory.com/91"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "kakaodeep",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "eunil_py38",
   "language": "python",
   "name": "eunil_py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
