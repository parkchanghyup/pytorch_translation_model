# torchtext 튜토리얼_한국어

## 훈련 데이터와 테스트 데이터 다운


```python
# 네이버 영화리뷰 데이터 다운
import urllib.request
import pandas as pd

urllib.request.urlretrieve("https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt", filename="ratings_train.txt")
urllib.request.urlretrieve("https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt", filename="ratings_test.txt")
```




    ('ratings_test.txt', <http.client.HTTPMessage at 0x7fea9e0a5128>)




```python
train_df = pd.read_table('ratings_train.txt')
test_df = pd.read_table('ratings_test.txt')
```


```python
print('훈련 데이터 샘플의 개수 : {}'.format(len(train_df)))
print('테스트 데이터 샘플의 개수 : {}'.format(len(test_df)))
```

    훈련 데이터 샘플의 개수 : 150000
    테스트 데이터 샘플의 개수 : 50000


## 필드 정의(torchtext.data)


```python
! pip install konlpy
```




```python

from torchtext.legacy import data 
from konlpy.tag import Okt
```


```python
tokenizer = Okt()
```


```python
# 필드 정의
ID = data.Field(sequential = False,
                use_vocab = False) # 실제 사용은 하지 않을 예정

TEXT = data.Field(sequential=True,
                  use_vocab=True,
                  tokenize=tokenizer.morphs, # 토크나이저로는 Okt 사용.
                  lower=True,
                  batch_first=True,
                  fix_length=20)

LABEL = data.Field(sequential=False,
                   use_vocab=False,
                   is_target=True)
```

## 데이터셋 만들기


```python
from torchtext.legacy.data import TabularDataset

train_data, test_data =TabularDataset.splits(
     path='.', train='ratings_train.txt', test='ratings_test.txt', format='tsv',
        fields=[('id', ID), ('text', TEXT), ('label', LABEL)], skip_header=True)

```


```python
print('훈련 샘플의 개수 : {}'.format(len(train_data)))
print('테스트 샘플의 개수 : {}'.format(len(test_data)))
```

    훈련 샘플의 개수 : 150000
    테스트 샘플의 개수 : 50000


## 단어 집합 만들기


```python
TEXT.build_vocab(train_data, min_freq = 10, max_size = 10000)
```


```python
print('단어 집합의 크기 : {}'.format(len(TEXT.vocab)))
```

    단어 집합의 크기 : 10002



```python
print(TEXT.vocab.stoi)
```

    defaultdict(<bound method Vocab._default_unk_index of <torchtext.legacy.vocab.Vocab object at 0x7fea3ac6d8d0>>, {'<unk>': 0, '<pad>': 1, '.': 2, '이': 3, '영화': 4, '의': 5, '..': 6, '가': 7, '에': 8, '을': 9, '...': 10, '도': 11, '들': 12, ',': 13, '는': 14, '를': 15, '은': 16, 
    ... 
    '다람쥐': 9996, '다우니': 9997, '다인': 9998, '단면': 9999, '단호': 10000, '닮음': 10001})


## 토치 텍스트의 데이터 로더 만들기


```python
from torchtext.legacy.data import Iterator
```


```python
# 하이퍼파라미터
batch_size = 128
lr = 0.001
EPOCHS = 10
```


```python
train_loader = Iterator(dataset = train_data, batch_size = batch_size)
test_loader = Iterator(dataset = test_data, batch_size = batch_size)
```


```python
print('훈련 데이터의 미니 배치 수 : {}'.format(len(train_loader)))
print('테스트 데이터의 미니 배치 수 : {}'.format(len(test_loader)))
```

    훈련 데이터의 미니 배치 수 : 1172
    테스트 데이터의 미니 배치 수 : 391


## RNN 모델 구현


```python
import os
import torch
import torch.nn as nn
import torch.nn.functional as F
from torchtext import data, datasets
import random
```


```python
class GRU(nn.Module):
    def __init__(self, n_layers, hidden_dim, n_vocab, embed_dim, n_classes, dropout_p=0.2):
        super(GRU, self).__init__()
        self.n_layers = n_layers
        self.hidden_dim = hidden_dim

        self.embed = nn.Embedding(n_vocab, embed_dim)
        self.dropout = nn.Dropout(dropout_p)
        self.gru = nn.GRU(embed_dim, self.hidden_dim,
                          num_layers=self.n_layers,
                          batch_first=True)
        self.out = nn.Linear(self.hidden_dim, n_classes)

    def forward(self, x):
        x = self.embed(x)
        h_0 = self._init_state(batch_size=x.size(0)) # 첫번째 히든 스테이트를 0벡터로 초기화
        x, _ = self.gru(x, h_0)  # GRU의 리턴값은 (배치 크기, 시퀀스 길이, 은닉 상태의 크기)
        h_t = x[:,-1,:] # 모든 문장을 거쳐서 나온 가장 마지막에 나온 단어(평점)의 값
        self.dropout(h_t)
        logit = self.out(h_t)  # (배치 크기, 은닉 상태의 크기) -> (배치 크기, 출력층의 크기)
        return logit

    def _init_state(self, batch_size=1):
        weight = next(self.parameters()).data
        return weight.new(self.n_layers, batch_size, self.hidden_dim).zero_()
```


```python
vocab_size = len(TEXT.vocab) 
n_classes = 2
print('단어 집합의 크기 : {}'.format(vocab_size))
print('클래스의 개수 : {}'.format(n_classes))
```

    단어 집합의 크기 : 10002
    클래스의 개수 : 2



```python
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = GRU(1, 256, vocab_size , 128, n_classes, 0.5).to(DEVICE)
optimizer = torch.optim.Adam(model.parameters(), lr=lr)
```


```python
def train(model, optimizer, train_iter):
    model.train()
    for i, batch in enumerate(train_iter):
        x, y = batch.text.to(DEVICE), batch.label.to(DEVICE)
        
        
        optimizer.zero_grad()

        logit = model(x)
        y= torch.tensor(y,dtype = torch.long,device =DEVICE)

        loss = F.cross_entropy(logit,y)
        loss.backward()
        optimizer.step()
```


```python
def evaluate(model, val_iter):
    """evaluate model"""
    model.eval()
    batch_cor, total_loss = 0, 0
    
    for batch in val_iter:
        x, y = batch.text.to(DEVICE), batch.label.to(DEVICE)
        y= torch.tensor(y,dtype = torch.long,device =DEVICE)
        logit = model(x)
        
        loss = F.cross_entropy(logit, y)
        
        total_loss += loss.item()
        batch_cor += (logit.max(1)[1] == y.data).sum() 
        
    size = len(val_iter.dataset)
    avg_loss = total_loss / size 
    avg_accuracy = 100.0 * batch_cor / size
    return avg_loss, avg_accuracy
```


```python
best_val_loss = None
best_model = []
EPOCHS= 10
for e in range(1, EPOCHS + 1 ):
    train(model, optimizer, train_loader)
    val_loss, val_accuracy = evaluate(model, test_loader)

    print("[Epoch: %d] val loss : %5.2f | val accuracy : %5.2f" % (e, val_loss, val_accuracy))

    # 검증 오차가 가장 적은 최적의 모델을 저장
    if not best_val_loss or val_loss < best_val_loss:
        best_val_loss = val_loss
        best_model = model
```

      


    [Epoch: 1] val loss :  0.01 | val accuracy : 82.48
    [Epoch: 2] val loss :  0.01 | val accuracy : 82.60
    [Epoch: 3] val loss :  0.01 | val accuracy : 82.45
    [Epoch: 4] val loss :  0.01 | val accuracy : 82.51
    [Epoch: 5] val loss :  0.01 | val accuracy : 82.60
    [Epoch: 6] val loss :  0.01 | val accuracy : 82.42
    [Epoch: 7] val loss :  0.01 | val accuracy : 82.61
    [Epoch: 8] val loss :  0.01 | val accuracy : 82.52
    [Epoch: 9] val loss :  0.01 | val accuracy : 82.37
    [Epoch: 10] val loss :  0.01 | val accuracy : 82.54

