{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['5_문어체_조례.xlsx',\n",
       " '2_대화체.xlsx',\n",
       " '1_구어체(2).xlsx',\n",
       " '1_구어체(1).xlsx',\n",
       " '3_문어체_뉴스(2).xlsx',\n",
       " '3_문어체_뉴스(3).xlsx',\n",
       " '3_문어체_뉴스(1).xlsx',\n",
       " '4_문어체_한국문화.xlsx',\n",
       " '3_문어체_뉴스(4).xlsx',\n",
       " '6_문어체_지자체웹사이트.xlsx']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = glob('*.xlsx')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 데이터 중 문어체_조레, 문어제_지자체웝사이트 파일 제외하고 데이터 프레임 생성\n",
    "\n",
    "df = pd.DataFrame(columns=['원문', '번역문'])\n",
    "\n",
    "file_list = ['2_대화체.xlsx',\n",
    "             '1_구어체(2).xlsx',\n",
    "             '1_구어체(1).xlsx',\n",
    "             '3_문어체_뉴스(2).xlsx',\n",
    "             '3_문어체_뉴스(3).xlsx',\n",
    "             '3_문어체_뉴스(1).xlsx',\n",
    "             '4_문어체_한국문화.xlsx',\n",
    "             '3_문어체_뉴스(4).xlsx']\n",
    "\n",
    "for data in file_list:\n",
    "    temp = pd.read_excel(data)\n",
    "    df = pd.concat([df, temp[['원문', '번역문']]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 불러오기\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchtext.datasets import TranslationDataset, Multi30k\n",
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 토크나이저 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import data\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "tokenizer = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_kor(text):\n",
    "    \"\"\"\n",
    "    한국어를 tokenizer해서 단어들을 리스트로 만든 후 reverse하여 반환\n",
    "    \"\"\"\n",
    "    return [text_ for text_ in tokenizer.morphs(text)][::-1]\n",
    "\n",
    "\n",
    "def tokenize_en(text):\n",
    "    \"\"\"\n",
    "    영어를 split tokenizer해서 단어들을 리스트로 만드는 함수\n",
    "\n",
    "    \"\"\"\n",
    "    return [text_ for text_ in text.split()]\n",
    "\n",
    "# 필드 정의\n",
    "\n",
    "\n",
    "SRC = data.Field(tokenize=tokenize_kor,\n",
    "                 init_token='<sos>',\n",
    "                 eos_token='<eos>', batch_first=True, lower=True)\n",
    "\n",
    "TRG = data.Field(tokenize=tokenize_en,\n",
    "                 init_token='<sos>',\n",
    "                 eos_token='<eos>', batch_first=True,\n",
    "                 lower=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터셋 만들기 (전체 데이터중 10만개만 사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shuffled = df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "# 우선 전제 데이터중 10만개만 사용\n",
    "df_ = df_shuffled[:100000]\n",
    "\n",
    "train_df = df_[:95000]\n",
    "test_df = df_[95000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trn size:  95000\n",
      "test size:  5000\n"
     ]
    }
   ],
   "source": [
    "print('trn size: ', len(train_df))\n",
    "print('test size: ', len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('train_df.csv', index=False)\n",
    "test_df.to_csv('test_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import TabularDataset\n",
    "\n",
    "train_data, test_data = TabularDataset.splits(\n",
    "    path='', train='train_df.csv', test='test_df.csv', format='csv',\n",
    "    fields=[('원문', SRC), ('번역문', TRG)], skip_header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 데이터와 검증데이터셋 분리\n",
    "train_data, validation_data = train_data.split(\n",
    "    split_ratio=0.8, random_state=random.seed(323))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 샘플의 개수 : 76000\n",
      "검증 샘플의 개수 : 19000\n"
     ]
    }
   ],
   "source": [
    "print('훈련 샘플의 개수 : {}'.format(len(train_data)))\n",
    "print('검증 샘플의 개수 : {}'.format(len(validation_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocab 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 말뭉치 생성\n",
    "SRC.build_vocab(train_data, min_freq=2)\n",
    "TRG.build_vocab(train_data, min_freq=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(SRC): 40291\n",
      "len(TRG): 39492\n"
     ]
    }
   ],
   "source": [
    "print(f\"len(SRC): {len(SRC.vocab)}\")\n",
    "print(f\"len(TRG): {len(TRG.vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "# data loader 생성\n",
    "from torchtext.data import Iterator\n",
    "\n",
    "# 하이퍼파라미터\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "batch_size = 128\n",
    "lr = 0.001\n",
    "EPOCHS = 20\n",
    "\n",
    "train_iterator = Iterator(dataset=train_data, batch_size=batch_size)\n",
    "valid_iterator = Iterator(dataset=validation_data, batch_size=batch_size)\n",
    "test_iterator = Iterator(dataset=test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터의 미니 배치 수 : 594\n",
      "검증 데이터의 미니 배치 수 : 149\n"
     ]
    }
   ],
   "source": [
    "print('훈련 데이터의 미니 배치 수 : {}'.format(len(train_iterator)))\n",
    "print('검증 데이터의 미니 배치 수 : {}'.format(len(valid_iterator)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# multi head attention\n",
    "1. query 행렬과 key 행렬간의 내적을 계산\n",
    "2. 1번 값에 key 행렬의 차원의 제곱근으로 나눔\n",
    "3. 2번 값에 소프트맥스 함수를 적용해 정규화 진행\n",
    "4. 3번 값에 value 행렬을 곱해 attention 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class MultiHeadAttentionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    encoder와 decoder의 multi head attention 부분\n",
    "\n",
    "    임베딩 된 sequence + positional encoding (or 이전 layer의 output) 을 이용해 \n",
    "    self attention 을 수행하고 다음 layer(residual, normalization)로 보냄\n",
    "\n",
    "    attribute \n",
    "    ---\n",
    "    hidden_dim : int\n",
    "        임베딩 차원\n",
    "    n_heads : int\n",
    "        헤드(head)의 개수: 서로 다른 어텐션(attention)의 수\n",
    "    dropout_ration : float\n",
    "        dropout 비율\n",
    "    device : \n",
    "        모델 학습과 추론에 사용할 device(gpu or cpu)\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_dim, n_heads, dropout_ratio, device):\n",
    "        super().__init__()\n",
    "\n",
    "        assert hidden_dim % n_heads == 0\n",
    "\n",
    "        self.hidden_dim = hidden_dim  # 임베딩 차원\n",
    "        self.n_heads = n_heads  # 헤드(head)의 개수: 서로 다른 어텐션(attention) 컨셉의 수\n",
    "        # 각 헤드(head)에서의 임베딩 차원 -> 각 헤드의 차원 * 헤드의 hidden dim  = input hidden dim 이여야함\n",
    "        self.head_dim = hidden_dim // n_heads\n",
    "\n",
    "        self.fc_q = nn.Linear(hidden_dim, hidden_dim)  # Query 값에 적용될 FC 레이어\n",
    "        self.fc_k = nn.Linear(hidden_dim, hidden_dim)  # Key 값에 적용될 FC 레이어\n",
    "        self.fc_v = nn.Linear(hidden_dim, hidden_dim)  # Value 값에 적용될 FC 레이어\n",
    "\n",
    "        self.fc_o = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "        파라미터 \n",
    "        ---\n",
    "\n",
    "        qurey :  [batch_size, query_len, hidden_dim]\n",
    "            input 데이터의 qurey 벡터\n",
    "        key :  [batch_size, query_len, hidden_dim]\n",
    "            input 데이터의 key 벡터\n",
    "        value :  [batch_size, query_len, hidden_dim]\n",
    "            input 데이터의 key 벡터\n",
    "        mask : boolen\n",
    "            모델 추론시 다음 단어 정보를 가리기위한 mask 행렬\n",
    "\n",
    "        returns \n",
    "        ---\n",
    "\n",
    "        x : [batch size, seq_len, hidden_size]\n",
    "            input과 차원이 같이야 여러 layer를 쌓을수 있음.\n",
    "        attention :\n",
    "            현재 layer의 attention score\n",
    "       \"\"\"\n",
    "\n",
    "        batch_size = query.shape[0]\n",
    "\n",
    "        Q = self.fc_q(query)\n",
    "        K = self.fc_k(key)\n",
    "        V = self.fc_v(value)\n",
    "\n",
    "        # Q: [batch_size, query_len, hidden_dim]\n",
    "        # K: [batch_size, key_len, hidden_dim]\n",
    "        # V: [batch_size, value_len, hidden_dim]\n",
    "\n",
    "        # hidden_dim → n_heads X head_dim 형태로 변형\n",
    "        # n_heads(h)개의 서로 다른 어텐션(attention) 컨셉을 학습하도록 유도\n",
    "        Q = Q.view(batch_size, -1, self.n_heads,\n",
    "                   self.head_dim).permute(0, 2, 1, 3)\n",
    "        K = K.view(batch_size, -1, self.n_heads,\n",
    "                   self.head_dim).permute(0, 2, 1, 3)\n",
    "        V = V.view(batch_size, -1, self.n_heads,\n",
    "                   self.head_dim).permute(0, 2, 1, 3)\n",
    "\n",
    "        # Q: [batch_size, n_heads, query_len, head_dim]\n",
    "        # K: [batch_size, n_heads, key_len, head_dim]\n",
    "        # V: [batch_size, n_heads, value_len, head_dim]\n",
    "\n",
    "        # Attention Energy 계산\n",
    "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / \\\n",
    "            self.scale  # 기울기 소실 방지\n",
    "\n",
    "        # energy: [batch_size, n_heads, query_len, key_len]\n",
    "\n",
    "        # 마스크(mask)를 사용하는 경우\n",
    "        if mask is not None:\n",
    "            # 마스크(mask) 값이 0인 부분을 -1e10으로 채우기\n",
    "            energy = energy.masked_fill(mask == 0, -1e10)\n",
    "\n",
    "        # 어텐션(attention) 스코어 계산: 각 단어에 대한 확률 값\n",
    "        attention = torch.softmax(energy, dim=-1)\n",
    "\n",
    "        # attention: [batch_size, n_heads, query_len, key_len]\n",
    "\n",
    "        # 여기에서 Scaled Dot-Product Attention을 계산\n",
    "        x = torch.matmul(self.dropout(attention), V)\n",
    "\n",
    "        # x: [batch_size, n_heads, query_len, head_dim]\n",
    "\n",
    "        x = x.permute(0, 2, 1, 3).contiguous()\n",
    "\n",
    "        # x: [batch_size, query_len, n_heads, head_dim]\n",
    "\n",
    "        x = x.view(batch_size, -1, self.hidden_dim)\n",
    "\n",
    "        # x: [batch_size, query_len, hidden_dim]\n",
    "\n",
    "        x = self.fc_o(x)\n",
    "\n",
    "        # x: [batch_size, query_len, hidden_dim]\n",
    "\n",
    "        return x, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feedforward layer\n",
    "피드포워드 레이어는 2개의 dense layer과 ReLU함수로 구성됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedforwardLayer(nn.Module):\n",
    "    \"\"\"\n",
    "\n",
    "    attribute \n",
    "    ---\n",
    "    hidden_dim : int\n",
    "        임베딩 차원\n",
    "    pf_dim : int\n",
    "        feedforwa layer시킬 차원\n",
    "    dropout_ration : float\n",
    "        dropout 비율\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_dim, pf_dim, dropout_ratio):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc_1 = nn.Linear(hidden_dim, pf_dim)\n",
    "        self.fc_2 = nn.Linear(pf_dim, hidden_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        파라미터 \n",
    "        ---\n",
    "        x : [batch_size, seq_len, hidden_dim]\n",
    "            multi head attention을 거쳐나온 output 값\n",
    "\n",
    "        returns \n",
    "        ---\n",
    "        x : [batch_size, seq_len, hidden_dim]\n",
    "            다음 layer로 보내기 값 출력\n",
    "        \"\"\"\n",
    "\n",
    "        x = self.dropout(torch.relu(self.fc_1(x)))\n",
    "\n",
    "        # x: [batch_size, seq_len, pf_dim]\n",
    "\n",
    "        x = self.fc_2(x)\n",
    "\n",
    "        # x: [batch_size, seq_len, hidden_dim]\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# transformer 의 encoder\n",
    "1. 입력 sequence를 임베딩 + positional embedding 값을 첫번째 인코더의 입력값으로 넣음\n",
    "2. 인코더 1은 입력값을 받아 multi head attention을 통해 attention 행렬 값 출력\n",
    "3. 어텐션 행렬을 FeedForward로 입력\n",
    "4. 그 다음 인코더 1의 출력값을 그 위에 있는 인코더(인코더 2)에 입력\n",
    "5. 인코더2는 위 과정을 반복."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder 내부를 구축하는 EncoderLayer\n",
    "\n",
    "    attribute \n",
    "    ---\n",
    "    hidden_dim : int\n",
    "        임베딩 차원\n",
    "    pf_dim : int\n",
    "        feedforward 시킬 차원\n",
    "    dropout_ration: float\n",
    "        dropout 비율\n",
    "    device : cuda or cpu\n",
    "        device 종류 \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_dim, n_heads, pf_dim, dropout_ratio, device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hidden_dim)\n",
    "        self.ff_layer_norm = nn.LayerNorm(hidden_dim)\n",
    "        self.self_attention = MultiHeadAttentionLayer(\n",
    "            hidden_dim, n_heads, dropout_ratio, device)\n",
    "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(\n",
    "            hidden_dim, pf_dim, dropout_ratio)\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        \"\"\"\n",
    "        파라미터\n",
    "        ---\n",
    "        src: [batch_size, src_len, hidden_dim]\n",
    "            input 데이터\n",
    "        src_mask: [batch_size, src_len]\n",
    "            필요한 경우 마스크(mask) 행렬을 이용하여 어텐션(attention)할 단어를 조절\n",
    "        \"\"\"\n",
    "\n",
    "        # self attention\n",
    "        # 하나의 입력값을 통해 query, key, value 값 생성 , decoder는 다름.\n",
    "        _src, _ = self.self_attention(src, src, src, src_mask)\n",
    "\n",
    "        # dropout, residual connection and layer norm\n",
    "        src = self.self_attn_layer_norm(src + self.dropout(_src))\n",
    "\n",
    "        # src: [batch_size, src_len, hidden_dim]\n",
    "\n",
    "        # position-wise feedforward\n",
    "        _src = self.positionwise_feedforward(src)\n",
    "\n",
    "        # dropout, residual and layer norm\n",
    "        src = self.ff_layer_norm(src + self.dropout(_src))\n",
    "\n",
    "        # src: [batch_size, src_len, hidden_dim]\n",
    "\n",
    "        return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    transformer 모델의 encoder\n",
    "    encoder layer를 여러층 쌓는다\n",
    "\n",
    "    attribute \n",
    "    ---\n",
    "    input_dim : int\n",
    "        input 데이터의 임베딩 차원    \n",
    "    hidden_dim : int\n",
    "        임베딩 차원\n",
    "    n_layers: int\n",
    "         encoder layer의 갯수\n",
    "    n_heads : int\n",
    "        multi head attention 의 head 갯수\n",
    "    pf_dim : int\n",
    "        feedforward 시킬 차원\n",
    "    dropout_ration: float\n",
    "        dropout 비율\n",
    "    device : cuda or cpu\n",
    "        device 종류 \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, n_layers, n_heads, pf_dim, dropout_ratio, device, max_length=100):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        self.tok_embedding = nn.Embedding(input_dim, hidden_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hidden_dim)\n",
    "        # layer를 쌓음\n",
    "        self.layers = nn.ModuleList([EncoderLayer(\n",
    "            hidden_dim, n_heads, pf_dim, dropout_ratio, device) for _ in range(n_layers)])\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([hidden_dim])).to(device)\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "\n",
    "        # src: [batch_size, src_len]\n",
    "        # src_mask: [batch_size, src_len]\n",
    "\n",
    "        batch_size = src.shape[0]\n",
    "        src_len = src.shape[1]\n",
    "\n",
    "        pos = torch.arange(0, src_len).unsqueeze(\n",
    "            0).repeat(batch_size, 1).to(self.device)\n",
    "\n",
    "        # pos: [batch_size, src_len]\n",
    "\n",
    "        # 소스 문장의 임베딩과 위치 임베딩을 더한 것을 사용\n",
    "        src = self.dropout(\n",
    "            (self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n",
    "\n",
    "        # src: [batch_size, src_len, hidden_dim]\n",
    "\n",
    "        # 모든 인코더 레이어를 차례대로 거치면서 순전파(forward) 수행\n",
    "        for layer in self.layers:\n",
    "            src = layer(src, src_mask)\n",
    "\n",
    "        # src: [batch_size, src_len, hidden_dim]\n",
    "\n",
    "        return src  # 마지막 레이어의 출력을 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# transformer의 decoder\n",
    "1. 디코더에 대한 입력 문장을 임베딩 행렬로 변환한 다음 위치 인코딩 정보를 추가하고 디코더(디코더 1)에 입력\n",
    "2. 디코더는 입력을 가져와 마스크된 멀티 헤드 어텐션 레이어에 보내고, 출력으로 어텐션 행렬 M을 반환\n",
    "3. 어텐션 행렬M, 인코딩 표현 R을 입력받아 멀티헤드 어텐션 레이어(인코더- 디코더 어텐션 레이어)에 값을 입력하고, 출력으로 새로운 어텐션 행렬 생성\n",
    "4. 어텐션 행렬을 피드포워드 레이어에 입력하여 값을 출력\n",
    "5. 디코더 1의 출력값을 다음 디코더(디코터 2)의 입력값으로 사용\n",
    "6. 디코더 2는 해당 내용 반복하며, 타깃 문장에 대한 디코더 표현을 반환."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    decoder 내부를 구축하는 decoderLayer\n",
    "\n",
    "    attribute \n",
    "    ---\n",
    "    hidden_dim : int\n",
    "        임베딩 차원\n",
    "    n_heads : int \n",
    "        multi head attention의 head의 갯수\n",
    "    pf_dim : int\n",
    "        feedforward 시킬 차원\n",
    "    dropout_ration: float\n",
    "        dropout 비율\n",
    "    device : cuda or cpu\n",
    "        device 종류 \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_dim, n_heads, pf_dim, dropout_ratio, device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hidden_dim)\n",
    "        self.enc_attn_layer_norm = nn.LayerNorm(hidden_dim)\n",
    "        self.ff_layer_norm = nn.LayerNorm(hidden_dim)\n",
    "        self.self_attention = MultiHeadAttentionLayer(\n",
    "            hidden_dim, n_heads, dropout_ratio, device)\n",
    "        self.encoder_attention = MultiHeadAttentionLayer(\n",
    "            hidden_dim, n_heads, dropout_ratio, device)\n",
    "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(\n",
    "            hidden_dim, pf_dim, dropout_ratio)\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "\n",
    "    # 인코더의 출력 값(enc_src)을 어텐션(attention)하는 구조\n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        \"\"\"\n",
    "        파라미터\n",
    "        ---\n",
    "        trg: [batch_size, trg_len, hidden_dim]\n",
    "            target 텍스트 (번역된 단어)\n",
    "\n",
    "        enc_src: [batch_size, src_len, hidden_dim]\n",
    "            encoder의 출력\n",
    "\n",
    "        trg_mask: [batch_size, trg_len]\n",
    "\n",
    "        src_mask: [batch_size, src_len]\n",
    "\n",
    "        returns \n",
    "        ---\n",
    "        trg : \n",
    "            번역된 단어 \n",
    "        attention : \n",
    "            attention 행렬        \n",
    "        \"\"\"\n",
    "\n",
    "        # self attention\n",
    "        # 자기 자신에 대하여 어텐션(attention)\n",
    "        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\n",
    "\n",
    "        # dropout, residual connection and layer norm\n",
    "        trg = self.self_attn_layer_norm(trg + self.dropout(_trg))\n",
    "\n",
    "        # trg: [batch_size, trg_len, hidden_dim]\n",
    "\n",
    "        # encoder attention\n",
    "        # 디코더의 쿼리(Query)를 이용해 인코더를 어텐션(attention)\n",
    "        _trg, attention = self.encoder_attention(\n",
    "            trg, enc_src, enc_src, src_mask)\n",
    "\n",
    "        # dropout, residual connection and layer norm\n",
    "        trg = self.enc_attn_layer_norm(trg + self.dropout(_trg))\n",
    "\n",
    "        # trg: [batch_size, trg_len, hidden_dim]\n",
    "\n",
    "        # positionwise feedforward\n",
    "        _trg = self.positionwise_feedforward(trg)\n",
    "\n",
    "        # dropout, residual and layer norm\n",
    "        trg = self.ff_layer_norm(trg + self.dropout(_trg))\n",
    "\n",
    "        # trg: [batch_size, trg_len, hidden_dim]\n",
    "        # attention: [batch_size, n_heads, trg_len, src_len]\n",
    "\n",
    "        return trg, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    transformer 모델의 Decoder\n",
    "    Decoder layer를 여러층 쌓는다\n",
    "\n",
    "    attribute \n",
    "    ---\n",
    "    output_dim : int\n",
    "        output 데이터의 임베딩 차원    \n",
    "    hidden_dim : int\n",
    "        임베딩 차원\n",
    "    n_layers: int\n",
    "         encoder layer의 갯수\n",
    "    n_heads : int\n",
    "        multi head attention 의 head 갯수\n",
    "    pf_dim : int\n",
    "        feedforward 시킬 차원\n",
    "    dropout_ration: float\n",
    "        dropout 비율\n",
    "    device : cuda or cpu\n",
    "        device 종류 \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_dim, hidden_dim, n_layers, n_heads, pf_dim, dropout_ratio, device, max_length=100):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        self.tok_embedding = nn.Embedding(output_dim, hidden_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hidden_dim)\n",
    "\n",
    "        self.layers = nn.ModuleList([DecoderLayer(\n",
    "            hidden_dim, n_heads, pf_dim, dropout_ratio, device) for _ in range(n_layers)])\n",
    "\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([hidden_dim])).to(device)\n",
    "\n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "\n",
    "        # trg: [batch_size, trg_len]\n",
    "        # enc_src: [batch_size, src_len, hidden_dim]\n",
    "        # trg_mask: [batch_size, trg_len]\n",
    "        # src_mask: [batch_size, src_len]\n",
    "\n",
    "        batch_size = trg.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "\n",
    "        pos = torch.arange(0, trg_len).unsqueeze(\n",
    "            0).repeat(batch_size, 1).to(self.device)\n",
    "\n",
    "        # pos: [batch_size, trg_len]\n",
    "\n",
    "        trg = self.dropout(\n",
    "            (self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\n",
    "\n",
    "        # trg: [batch_size, trg_len, hidden_dim]\n",
    "\n",
    "        for layer in self.layers:\n",
    "            # 소스 마스크와 타겟 마스크 모두 사용\n",
    "            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\n",
    "\n",
    "        # trg: [batch_size, trg_len, hidden_dim]\n",
    "        # attention: [batch_size, n_heads, trg_len, src_len]\n",
    "\n",
    "        output = self.fc_out(trg)\n",
    "\n",
    "        # output: [batch_size, trg_len, output_dim]\n",
    "\n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    taransformer 모델 \n",
    "    encoder와 decoder로 구성됨\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoder, decoder, src_pad_idx, trg_pad_idx, device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "\n",
    "    # 소스 문장의 <pad> 토큰에 대하여 마스크(mask) 값을 0으로 설정\n",
    "    def make_src_mask(self, src):\n",
    "\n",
    "        # src: [batch_size, src_len]\n",
    "\n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        # src_mask: [batch_size, 1, 1, src_len]\n",
    "\n",
    "        return src_mask\n",
    "\n",
    "    # 타겟 문장에서 각 단어는 다음 단어가 무엇인지 알 수 없도록(이전 단어만 보도록) 만들기 위해 마스크를 사용\n",
    "    def make_trg_mask(self, trg):\n",
    "\n",
    "        # trg: [batch_size, trg_len]\n",
    "        \"\"\" (마스크 예시)\n",
    "        1 0 0 0 0\n",
    "        1 1 0 0 0\n",
    "        1 1 1 0 0\n",
    "        1 1 1 0 0\n",
    "        1 1 1 0 0\n",
    "        \"\"\"\n",
    "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        # trg_pad_mask: [batch_size, 1, 1, trg_len]\n",
    "\n",
    "        trg_len = trg.shape[1]\n",
    "\n",
    "        \"\"\" (마스크 예시)\n",
    "        1 0 0 0 0\n",
    "        1 1 0 0 0\n",
    "        1 1 1 0 0\n",
    "        1 1 1 1 0\n",
    "        1 1 1 1 1\n",
    "        \"\"\"\n",
    "        trg_sub_mask = torch.tril(torch.ones(\n",
    "            (trg_len, trg_len), device=self.device)).bool()\n",
    "\n",
    "        # trg_sub_mask: [trg_len, trg_len]\n",
    "\n",
    "        trg_mask = trg_pad_mask & trg_sub_mask\n",
    "\n",
    "        # trg_mask: [batch_size, 1, trg_len, trg_len]\n",
    "\n",
    "        return trg_mask\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "\n",
    "        # src: [batch_size, src_len]\n",
    "        # trg: [batch_size, trg_len]\n",
    "\n",
    "        src_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "\n",
    "        # src_mask: [batch_size, 1, 1, src_len]\n",
    "        # trg_mask: [batch_size, 1, trg_len, trg_len]\n",
    "\n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "\n",
    "        # enc_src: [batch_size, src_len, hidden_dim]\n",
    "\n",
    "        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
    "\n",
    "        # output: [batch_size, trg_len, output_dim]\n",
    "        # attention: [batch_size, n_heads, trg_len, src_len]\n",
    "\n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 학습\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼 파라미터 설정\n",
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "HIDDEN_DIM = 256\n",
    "ENC_LAYERS = 3\n",
    "DEC_LAYERS = 3\n",
    "ENC_HEADS = 8\n",
    "DEC_HEADS = 8\n",
    "ENC_PF_DIM = 512\n",
    "DEC_PF_DIM = 512\n",
    "ENC_DROPOUT = 0.1\n",
    "DEC_DROPOUT = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 정의\n",
    "SRC_PAD_IDX = SRC.vocab.stoi[SRC.pad_token]\n",
    "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:1')\n",
    "# 인코더(encoder)와 디코더(decoder) 객체 선언\n",
    "enc = Encoder(INPUT_DIM, HIDDEN_DIM, ENC_LAYERS,\n",
    "              ENC_HEADS, ENC_PF_DIM, ENC_DROPOUT, device)\n",
    "dec = Decoder(OUTPUT_DIM, HIDDEN_DIM, DEC_LAYERS,\n",
    "              DEC_HEADS, DEC_PF_DIM, DEC_DROPOUT, device)\n",
    "\n",
    "\n",
    "# Transformer 객체 선언\n",
    "model = Transformer(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가중치 초기화\n",
    "def initialize_weights(m):\n",
    "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "        nn.init.xavier_uniform_(m.weight.data)\n",
    "\n",
    "\n",
    "model.apply(initialize_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Adam optimizer로 학습 최적화\n",
    "LEARNING_RATE = 0.0005\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# 뒷 부분의 패딩(padding)에 대해서는 값 무시\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=TRG_PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \"\"\"\n",
    "    모델 학습 함수\n",
    "    \"\"\"\n",
    "    model.train()  # 학습 모드\n",
    "    epoch_loss = 0\n",
    "\n",
    "    # 전체 학습 데이터를 확인하며\n",
    "    for i, batch in enumerate(iterator):\n",
    "\n",
    "        src = batch.원문.to(device)\n",
    "        trg = batch.번역문.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 출력 단어의 마지막 인덱스(<eos>)는 제외\n",
    "        # 입력을 할 때는 <sos>부터 시작하도록 처리\n",
    "        output, _ = model(src, trg[:, :-1])\n",
    "\n",
    "        # output: [배치 크기, trg_len - 1, output_dim]\n",
    "        # trg: [배치 크기, trg_len]\n",
    "\n",
    "        output_dim = output.shape[-1]\n",
    "\n",
    "        output = output.contiguous().view(-1, output_dim)\n",
    "        # 출력 단어의 인덱스 0(<sos>)은 제외\n",
    "        trg = trg[:, 1:].contiguous().view(-1)\n",
    "\n",
    "        # output: [배치 크기 * trg_len - 1, output_dim]\n",
    "        # trg: [배치 크기 * trg len - 1]\n",
    "\n",
    "        # 모델의 출력 결과와 타겟 문장을 비교하여 손실 계산\n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()  # 기울기(gradient) 계산\n",
    "\n",
    "        # 기울기(gradient) clipping 진행\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        # 파라미터 업데이트\n",
    "        optimizer.step()\n",
    "\n",
    "        # 전체 손실 값 계산\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \"\"\"\n",
    "    모델 평가 함수\n",
    "    \"\"\"\n",
    "    model.eval()  # 평가 모드\n",
    "    epoch_loss = 0\n",
    "\n",
    "    # 전체 평가 데이터를 확인하며\n",
    "    for i, batch in enumerate(iterator):\n",
    "        src = batch.원문.to(device)\n",
    "        trg = batch.번역문.to(device)\n",
    "\n",
    "        # 출력 단어의 마지막 인덱스(<eos>)는 제외\n",
    "        # 입력을 할 때는 <sos>부터 시작하도록 처리\n",
    "        output, _ = model(src, trg[:, :-1])\n",
    "\n",
    "        # output: [배치 크기, trg_len - 1, output_dim]\n",
    "        # trg: [배치 크기, trg_len]\n",
    "\n",
    "        output_dim = output.shape[-1]\n",
    "\n",
    "        output = output.contiguous().view(-1, output_dim)\n",
    "        # 출력 단어의 인덱스 0(<sos>)은 제외\n",
    "        trg = trg[:, 1:].contiguous().view(-1)\n",
    "\n",
    "        # output: [배치 크기 * trg_len - 1, output_dim]\n",
    "        # trg: [배치 크기 * trg len - 1]\n",
    "\n",
    "        # 모델의 출력 결과와 타겟 문장을 비교하여 손실 계산\n",
    "        loss = criterion(output, trg)\n",
    "\n",
    "        # 전체 손실 값 계산\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 1m 15s\n",
      "\tTrain Loss: 6.103 | Train PPL: 447.402\n",
      "\tValidation Loss: 5.132 | Validation PPL: 169.315\n",
      "Epoch: 02 | Time: 1m 15s\n",
      "\tTrain Loss: 4.932 | Train PPL: 138.722\n",
      "\tValidation Loss: 4.632 | Validation PPL: 102.688\n",
      "Epoch: 03 | Time: 1m 15s\n",
      "\tTrain Loss: 4.327 | Train PPL: 75.738\n",
      "\tValidation Loss: 4.294 | Validation PPL: 73.234\n",
      "Epoch: 04 | Time: 1m 15s\n",
      "\tTrain Loss: 3.801 | Train PPL: 44.756\n",
      "\tValidation Loss: 4.110 | Validation PPL: 60.972\n",
      "Epoch: 05 | Time: 1m 15s\n",
      "\tTrain Loss: 3.356 | Train PPL: 28.684\n",
      "\tValidation Loss: 4.015 | Validation PPL: 55.427\n",
      "Epoch: 06 | Time: 1m 15s\n",
      "\tTrain Loss: 2.975 | Train PPL: 19.592\n",
      "\tValidation Loss: 4.006 | Validation PPL: 54.931\n",
      "Epoch: 07 | Time: 1m 15s\n",
      "\tTrain Loss: 2.646 | Train PPL: 14.104\n",
      "\tValidation Loss: 4.042 | Validation PPL: 56.942\n",
      "Epoch: 08 | Time: 1m 15s\n",
      "\tTrain Loss: 2.367 | Train PPL: 10.662\n",
      "\tValidation Loss: 4.099 | Validation PPL: 60.308\n",
      "Epoch: 09 | Time: 1m 15s\n",
      "\tTrain Loss: 2.129 | Train PPL: 8.403\n",
      "\tValidation Loss: 4.194 | Validation PPL: 66.292\n",
      "Epoch: 10 | Time: 1m 15s\n",
      "\tTrain Loss: 1.930 | Train PPL: 6.890\n",
      "\tValidation Loss: 4.295 | Validation PPL: 73.315\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time()  # 시작 시간 기록\n",
    "\n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "\n",
    "    end_time = time.time()  # 종료 시간 기록\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'transformer_german_to_english.pt')\n",
    "\n",
    "    print(f'Epoch: {epoch + 1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(\n",
    "        f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):.3f}')\n",
    "    print(\n",
    "        f'\\tValidation Loss: {valid_loss:.3f} | Validation PPL: {math.exp(valid_loss):.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
